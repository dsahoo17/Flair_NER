{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoxlDAnvNBWV4l6yZfpd6C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsahoo17/Flair_NER/blob/master/cellphone_reviews_TBSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#URL used: https://www.kaggle.com/datasets/grikomsn/amazon-cell-phones-reviews\n",
        "\n",
        "project_directory = \"phone_review\"\n",
        "\n",
        "from os.path import exists\n",
        "import os\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if not exists(project_directory):\n",
        "  os.mkdir(project_directory)\n",
        "\n",
        "os.chdir(\"/content/\"+project_directory)\n",
        "\n",
        "#upload file to phone_review"
      ],
      "metadata": {
        "id": "diVQYOHUGTEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "id": "uSXbYrtealuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"cellphone_reviews.zip\", \"./\")"
      ],
      "metadata": {
        "id": "HiE1d2lg2Jyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload kaggle api key\n",
        "# ! cp ../kaggle.json /root/.kaggle\n",
        "# !kaggle datasets download -d grikomsn/amazon-cell-phones-reviews"
      ],
      "metadata": {
        "id": "IjOeEnbXOwKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### dependency ###\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "!pip install langdetect\n",
        "!pip install aspect_based_sentiment_analysis\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "!pip install spacy\n",
        "!spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "2sm5dQ1xQ0ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read Cellphone reviews data\n",
        "import pandas as pd\n",
        "items_df = pd.read_csv(r\"20191226-items.csv\",encoding='utf-8')\n",
        "reviews_df = pd.read_csv(r\"20191226-reviews.csv\",encoding='utf-8')"
      ],
      "metadata": {
        "id": "KJbUUJseHS2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_sample_df = reviews_df[[\"asin\",\"body\"]]\n",
        "items_sample_df = items_df[[\"asin\",\"brand\",\"title\"]]\n",
        "#Merge based on common column \"asin\"\n",
        "data_df = pd.merge(reviews_sample_df, items_sample_df, on=\"asin\")"
      ],
      "metadata": {
        "id": "1j-3GKHkINKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_df.copy(deep=True)"
      ],
      "metadata": {
        "id": "UVEMkryf4agy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(2)"
      ],
      "metadata": {
        "id": "kIWnWTpLOKoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "en_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "ignore_stopwords = ['no','not',\"doesn't\",\"didn't\",\"couldn't\",\"isn't\"]\n",
        "for iw in ignore_stopwords:\n",
        "    en_stopwords.remove(iw)\n",
        "\n",
        "def pre_process(text):\n",
        "    text = str(text).lower()\n",
        "    sttt = []\n",
        "    for p in '!\"#%&\\'()*+\\,-./:;<=>?@[\\\\]^_`{|}~':\n",
        "        text = text.replace(p,\"\")\n",
        "    for word in text.split(\" \"):\n",
        "        if word not in en_stopwords or \"n't\" in word:\n",
        "            sttt.append(word)\n",
        "    return (\" \").join(sttt)\n",
        "\n",
        "data[\"cleaned_body\"] = data[\"body\"].apply(pre_process).str.replace(r' +',' ',regex=True)"
      ],
      "metadata": {
        "id": "RrZS9A1GTC6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "# data[\"language\"] = data[\"body\"].apply(detect)\n",
        "language_lst = []\n",
        "for index,row in data.iterrows():\n",
        "    if index%2000 == 0:\n",
        "        print(\"index:\",index)\n",
        "    try:\n",
        "        language = detect(row[\"cleaned_body\"])\n",
        "    except:\n",
        "        language = \"language_not_detected\"\n",
        "        # print(\"Error! Language not detected in:\", row[\"cleaned_body\"])\n",
        "    language_lst.append(language)\n",
        "data[\"language\"]  = language_lst"
      ],
      "metadata": {
        "id": "Hh5O3kiTQ0o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data[\"language\"]=='en']\n",
        "data = data[[\"asin\",\"body\",\"cleaned_body\"]]"
      ],
      "metadata": {
        "id": "BhQID9YkSz6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aspect_based_sentiment_analysis as absa"
      ],
      "metadata": {
        "id": "Z1ZozScpWvS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = absa.load()"
      ],
      "metadata": {
        "id": "PLnz-FBwW3iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"tokenized_body\"] = data['cleaned_body'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "DppYLF9qPSPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "UtK7T3P2PfJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_nlp = spacy.load(\"en_core_web_md\")\n",
        "def spacy_pre_process(txt):\n",
        "    req_tag = ['NN']\n",
        "    extracted_nouns = []\n",
        "    tokenized_body = []\n",
        "    try:\n",
        "        doc = spacy_nlp(txt)\n",
        "        for token in doc:\n",
        "            tokenized_body.append(token.lemma_)\n",
        "            if token.tag_ in req_tag:\n",
        "                extracted_nouns.append(token.lemma_)\n",
        "        return pd.Series([list(set(extracted_nouns)),tokenized_body])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return pd.Series([list(set(extracted_nouns)),tokenized_body])"
      ],
      "metadata": {
        "id": "AICRLKUoRkyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[[\"Spacy_Nouns\",\"tokenized_body\"]] =  data[\"cleaned_body\"].apply(spacy_pre_process)\n"
      ],
      "metadata": {
        "id": "cSHKdZOaStGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "Qy3Jo29o0Z-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_noun_list = []\n",
        "for idx,row in data.Spacy_Nouns.iteritems():\n",
        "    for r in row:\n",
        "        all_noun_list.append(r)"
      ],
      "metadata": {
        "id": "cMXxRzCqtEA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counts = Counter(all_noun_list)\n",
        "freq_list = sorted(counts.items(), key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "REpQGLC8uP8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_list[0:20]"
      ],
      "metadata": {
        "id": "TII5tHlsvJLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspects = [\"phone\",\"battery\",\"screen\",\"camera\",\"price\",\"charger\"]"
      ],
      "metadata": {
        "id": "3SI1p8Megvbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(\"./data_full_nouns.csv\")"
      ],
      "metadata": {
        "id": "uQC6t25mHZHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "id": "e3tWWuVPPAPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "\n",
        "# Load the binary model\n",
        "model = word2vec.Word2Vec(data['tokenized_body'],workers=10,\n",
        "                          size=300,min_count=2,window=10)"
      ],
      "metadata": {
        "id": "P6fSLMSZJabB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(\"bluetooth\")"
      ],
      "metadata": {
        "id": "Tle2l6GGkXrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(\"smartphone\")"
      ],
      "metadata": {
        "id": "-g2HZ0NUO6bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = \"camera is good but battery is draining so fast.\"\n",
        "sc = \"I had the Samsung A600 for awhile which is absolute doo doo. You can read my review on it and detect my rage at the stupid thing.\"\n",
        "aspects = [\"phone\",\"battery\",\"screen\",\"camera\",\"price\",\"charger\"]\n",
        "\n",
        "def calculate_aspect(txt):\n",
        "  TH = 0.6\n",
        "  result = {}\n",
        "  # for a in aspects:\n",
        "  #   result[a]=\"neutral\"\n",
        "  txt = txt.lower()\n",
        "  # print(result)\n",
        "  for items in list(nlp(txt, aspects=aspects).subtasks.items()):\n",
        "      # print(items[0],items[1].sentiment.name, items[1].scores)\n",
        "      if max(items[1].scores) > TH:\n",
        "          result[str(items[0])] = items[1].sentiment.name\n",
        "  return result\n",
        "\n",
        "print(calculate_aspect(sc))\n"
      ],
      "metadata": {
        "id": "LxIgWH7KPua9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_bkp = data.sample(frac=0.0001)\n",
        "data_bkp[\"aspect\"] = data_bkp[\"cleaned_body\"].apply(calculate_aspect)"
      ],
      "metadata": {
        "id": "nT6J9Xw8qEDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_bkp.head(100)"
      ],
      "metadata": {
        "id": "pbsn4CRjqe-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_bkp.body.iloc[0]"
      ],
      "metadata": {
        "id": "MnqiUXtOqli2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "15jCEVzdqll9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRr-TzHEqlpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "sp = spacy.load(\"en_core_web_md\")\n",
        "from textblob import TextBlob\n",
        "  \n",
        "# Creating a list of positive and negative sentences.\n",
        "mixed_sen = [\n",
        "  \"camera is good but battery is draining so fast.\"\n",
        "]\n",
        "  \n",
        "# An empty list for obtaining the extracted aspects\n",
        "# from sentences. \n",
        "ext_aspects = []\n",
        "  \n",
        "# Performing Aspect Extraction\n",
        "for sen in mixed_sen:\n",
        "  important = sp(sen)\n",
        "  descriptive_item = ''\n",
        "  target = ''\n",
        "  for token in important:\n",
        "    if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
        "      target = token.text\n",
        "    if token.pos_ == 'ADJ':\n",
        "      added_terms = ''\n",
        "      for mini_token in token.children:\n",
        "        if mini_token.pos_ != 'ADV':\n",
        "          continue\n",
        "        added_terms += mini_token.text + ' '\n",
        "      descriptive_item = added_terms + token.text\n",
        "  ext_aspects.append({'aspect': target,\n",
        "    'description': descriptive_item})\n",
        "  \n",
        "print(\"ASPECT EXTRACTION\\n\")\n",
        "print(ext_aspects)\n",
        "  \n",
        "  \n",
        "for aspect in ext_aspects:\n",
        "  aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n",
        "  \n",
        "print(\"\\n\")\n",
        "print(\"SENTIMENT ASSOCIATION\\n\")\n",
        "print(ext_aspects)"
      ],
      "metadata": {
        "id": "A1K4d7lyh1N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_-02U4Zh3IU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}